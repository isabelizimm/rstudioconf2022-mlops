---
format: 
    revealjs:
        slide-number: true
        theme: [default, style.scss]
        logo: images/vetiverhex.png
        width: 1600
        height: 900
---
##

::: {style="position: absolute; left: 480px; top: 200px; height: 525px; width: 1500px; background-color: #f3cbac; padding: 20px; padding-left: 50px; box-shadow: 15px 15px 0px 0px #a46848; border-radius: 5px;"}
[Demystifying MLOps]{style="font-size: 120px; font-weight: bold; line-height: 1em; margin: 0px"}

<br>

[Isabel Zimmerman, RStudio PBC]{style="font-size: 60px; font-weight: bold;"}

[rstudio::conf(2022)]{style="font-size: 50px;font-weight: bold;"}
:::

::: {.notes}
im here to demystfy mlops because machine learning operations are HARD i spent a lot of time with deploying models,,,

more importantly I LOVE BAKING
:::

##

![](images/happy.jpg){height="700" margin-left="700" margin-right="auto"}

::: {.notes}
machine learning models are like chocolate chips...

while model development occurs in notebooks, the value of models often comes when they are integrated into a larger system
:::

## what is MLOps?

set of <u>practices</u> to _deploy_ and _maintain_ machine learning models in production **reliably** and **efficiently**

::: {.notes}
what are some MLOps practices?
:::

##

![](images/evolution_of_cookie.png)

::: {.notes}
- write down our recipe
- baking the dough
- making changes when things don't go as planned
:::

##

![](images/ml_ops_cycle.png)

::: {.notes}
- version
- deploy
- monitor

now, let's pause here for a moment-- MLOps is not a disjoint piece where data scientists can pass off models to IT, or whoever handles deployment. MLOps is part of the data science lifecycle.

There are lots of tools for the righthand side of this image--if you were just at Julia and Max's keynote, this might look familiar. Well, mostly familiar.
:::


## version
_how do we track and manage change?_

![](images/recipe_card.png)

::: {.notes}
try different flours, using salted or unsalted butter,

quick cautionary tale
:::

## version

`model`

. . .

`model_final`

. . .

`model_final_ACTUALLY`

. . .

`model_final_ACTUALLY_1`

::: {.notes}
lacks context
not scalable
:::
   
## version

versioning is useful to track changes across _time_, but it should also be for **different implementations**

::: {.notes}
you might have to think more deeply about your versioning if

for time: if you think to yourself "where is that model?" or "oops, just downloaded the wrong model (again)" or 

different implementations: if you have a staging and production model. if you 

- keep track of your models
- choose what model is in production
    - you want a structure that you can quickly and easily change model versions 
- model registries (centralized location to store/version models) are great for versioning AND SHARING models with your team

:::

## version::vetiver

:::: {.columns}

::: {.column width="50%"}
_in r_
```r
library(vetiver)
library(pins)

model_board <- board_temp()

v = vetiver_model(model, "model_name")
model_board %>% vetiver_pin_write(v)
```
:::

::: {.column width="50%"}
_in python_
```python
import vetiver
import pins

model_board = board_temp()

v = VetiverModel(model, 
    "model_name", 
    ptype_data = mtcars)

vetiver_pin_write(model_board, v)
```
:::

::::

::: {.notes}
okay, well, how did that solve our problems with scale and context?
:::

## version::vetiver

```r

```

```
api_version: 1
created: 20220719T142221Z
description: Scikit-learn  model
file: cars_linear.joblib
file_size: 1087
pin_hash: 4db397b49e7bff0b
title: 'cars_linear: a pinned LinearRegression object'
type: joblib
user:
  ptype: '{"cyl": 6.0, "disp": 160.0, "hp": 110.0, "drat": 3.9, "wt": 2.62, "qsec":
    16.46, "vs": 0.0, "am": 1.0, "gear": 4.0, "carb": 4.0}'
  required_pkgs:
  - vetiver
  - scikit-learn
```
::: {.notes}
okay, well, how did that solve our problems with scale and context?
:::
## deploy

![](images/happy.jpg){height="700" margin-left="700" margin-right="auto"}

::: {.notes}
when building vetiver, we had to choose WHERE to deploy these models. there's a few options, so let's go on a quick journey to see how we made our decision.
:::

## deploy

- in XML (with PMML)
    - flexible in integration 
    - *not* flexible in modeling

::: {.notes}

XML is best known for displaying documents on the internet
    - baking cookies on an open flame (image)
    - models in XML (image?)
:::

## deploy

- ~~in XML (with PMML)~~
- in databases (with SQL stored procedures)

    - flexible in modeling
    - *not* flexible in integration


::: {.notes}
- smaller
    - baking cookies in a waffle iron (image)
    - models saved in a database
    - works best if your workflow is centered around a database that you can easily interact with
    - not always as accessible to non-technical team members

:::

## deploy

- ~~in XML (with PMML)~~
- ~~in databases (with SQL stored procedures)~~
- in an API (with RESTful APIs)

    - highly flexible in modeling
    - highly flexible in integration

::: {.notes}

this can be a rest api on your computer, in the cloud ie, SAGEMAKER, ETC, if you're more interested in this, James Blair is hosting ____

if youre interested in seeing this in RSConnnect, gagan and xu fei are talking 
standard, easy
can deploy anything you write in R or Python
POST/GET/QUERY to these endpoints

    - baking cookies in an oven (image)
    - models living in application interfaces
    - easy to use with other tools, so you keep the same workflow
    - often comes with visual documentation to be accessible to many different skill levels
    - interact in the browser to debug
    - straightforward to put inside docker containers

:::


## vetiver

:::: {.columns}

::: {.column width="50%"}
_in python_
```python
api = VetiverAPI(v)
api.run()
```
:::

::: {.column width="50%"}
_in r_
```r
library(plumber)

pr() %>%
  vetiver_api(v)
```
:::

::::

## vetiver

![](images/visualapi.png)

## monitor

![](images/burnt.jpg)

:::{.notes}
have a plan for how long your cookies are in the oven, but you have to keep an eye on them and adjust your plan if things aren't working out
:::

## monitor

- monitor your data
    - does your data from when you trained your model 2 months ago look the same as today?

## monitor

- monitor your data (data drift)
- monitor model performance (model drift)
    - models fail silently! and they can still run with no error, even if your accuracy is zero percent
        more detail here
    - i listened to waaaayyyy more jonas brothers in 2012 than i do now, and my spotify recommendation model has had to adapt!


## monitor

- monitor your data
- monitor model performance
- know what to do with degredation
    - back to model versioning!
    - retraining model?
    - new model type altogether?

## vetiver

:::: {.columns}

::: {.column width="50%"}
_in python_
```python
metrics = vetiver.compute_metrics(
    data = new_cars, 
    date_var = "date_obs", 
    period = td, 
    metric_set = metric_set, 
    truth = "mpg", 
    estimate = "preds"
    )
                    
vetiver.pin_metrics(
    board = model_board, 
    df_metrics = metrics, 
    metrics_pin_name = "tree_metrics", 
    overwrite = True
    )

vetiver.plot_metrics(
    df_metrics = metrics
    )
```
:::

::: {.column width="50%"}
_in r_
```r
metrics <-
    augment(v, new_data = new_cars) %>%
    vetiver_compute_metrics(
        date_obs, 
        "week", 
        mpg, 
        .pred)

model_board %>%
    vetiver_pin_metrics(new_metrics, "tree_metrics", overwrite = TRUE)

```
:::

::::

## putting it all together

best practices:

- version your model
- deploy your model
- monitor your model

... 

but also!

- responsible reporting
- data validation
- know the steps to update a model

## what now?

::: {.notes}
- think of what you need from MLOps
    - what is your strategy now? where can you add in best practices to make it better?

check out vetiver in PYTHON OR R in the open source or pro products lounge
:::

